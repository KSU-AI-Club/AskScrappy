{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e2675a-61ac-469e-bcfa-bb17e7979fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List\n",
    "from langchain_community.document_loaders import UnstructuredLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import time\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1cde9e8-d0c1-4298-8b6e-cd6142be201d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['USER_AGENT'] = 'scrappy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fdb9cc8-eb97-4e31-8513-514aac8e35bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader, DirectoryLoader, PyMuPDFLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_ollama import OllamaLLM\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4033e7-c03f-40f7-a385-ecf5ca265186",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d71d3b47-dcd5-46cd-90ad-63ed386fae4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped 8 unique professor IDs so far...\n",
      "Scraped 16 unique professor IDs so far...\n",
      "Scraped 24 unique professor IDs so far...\n",
      "Scraped 31 unique professor IDs so far...\n",
      "Scraped 39 unique professor IDs so far...\n",
      "Scraped 47 unique professor IDs so far...\n",
      "Scraped 55 unique professor IDs so far...\n",
      "Scraped 63 unique professor IDs so far...\n",
      "Scraped 71 unique professor IDs so far...\n",
      "Scraped 79 unique professor IDs so far...\n",
      "Scraped 87 unique professor IDs so far...\n",
      "Scraped 95 unique professor IDs so far...\n",
      "Scraped 103 unique professor IDs so far...\n",
      "Scraped 110 unique professor IDs so far...\n",
      "Scraped 118 unique professor IDs so far...\n",
      "Scraped 126 unique professor IDs so far...\n",
      "Scraped 133 unique professor IDs so far...\n",
      "Scraped 141 unique professor IDs so far...\n",
      "Scraped 149 unique professor IDs so far...\n",
      "Scraped 157 unique professor IDs so far...\n",
      "Scraped 165 unique professor IDs so far...\n",
      "Scraped 173 unique professor IDs so far...\n",
      "Scraped 180 unique professor IDs so far...\n",
      "Scraped 188 unique professor IDs so far...\n",
      "Scraped 195 unique professor IDs so far...\n",
      "Scraped 200 unique professor IDs so far...\n",
      "Scraped 208 unique professor IDs so far...\n",
      "Scraped 216 unique professor IDs so far...\n",
      "Scraped 223 unique professor IDs so far...\n",
      "Scraped 231 unique professor IDs so far...\n",
      "Scraped 239 unique professor IDs so far...\n",
      "Scraped 246 unique professor IDs so far...\n",
      "Scraped 254 unique professor IDs so far...\n",
      "Scraped 262 unique professor IDs so far...\n",
      "Scraped 270 unique professor IDs so far...\n",
      "Scraped 275 unique professor IDs so far...\n",
      "Scraped 283 unique professor IDs so far...\n",
      "Scraped 290 unique professor IDs so far...\n",
      "Scraped 298 unique professor IDs so far...\n",
      "Scraped 306 unique professor IDs so far...\n",
      "Scraped 309 unique professor IDs so far...\n",
      "Scraped 316 unique professor IDs so far...\n",
      "Scraped 322 unique professor IDs so far...\n",
      "Scraped 330 unique professor IDs so far...\n",
      "Scraped 337 unique professor IDs so far...\n",
      "Scraped 344 unique professor IDs so far...\n",
      "Scraped 352 unique professor IDs so far...\n",
      "Scraped 360 unique professor IDs so far...\n",
      "Scraped 368 unique professor IDs so far...\n",
      "Scraped 376 unique professor IDs so far...\n",
      "Scraped 384 unique professor IDs so far...\n",
      "Scraped 392 unique professor IDs so far...\n",
      "Scraped 400 unique professor IDs so far...\n",
      "Scraped 408 unique professor IDs so far...\n",
      "Scraped 415 unique professor IDs so far...\n",
      "Scraped 423 unique professor IDs so far...\n",
      "Scraped 429 unique professor IDs so far...\n",
      "Scraped 433 unique professor IDs so far...\n",
      "Scraped 441 unique professor IDs so far...\n",
      "Scraped 446 unique professor IDs so far...\n",
      "Scraped 450 unique professor IDs so far...\n",
      "Scraped 456 unique professor IDs so far...\n",
      "Scraped 463 unique professor IDs so far...\n",
      "Scraped 471 unique professor IDs so far...\n",
      "Scraped 479 unique professor IDs so far...\n",
      "Scraped 484 unique professor IDs so far...\n",
      "Scraped 491 unique professor IDs so far...\n",
      "Scraped 497 unique professor IDs so far...\n",
      "A timeout occurred: Message: script timeout\n",
      "  (Session info: chrome=129.0.6668.89)\n",
      "Stacktrace:\n",
      "#0 0x5db2d5e3302a <unknown>\n",
      "#1 0x5db2d5b1943d <unknown>\n",
      "#2 0x5db2d5badd33 <unknown>\n",
      "#3 0x5db2d5b8db22 <unknown>\n",
      "#4 0x5db2d5bacd7d <unknown>\n",
      "#5 0x5db2d5b8d8c3 <unknown>\n",
      "#6 0x5db2d5b5b6b3 <unknown>\n",
      "#7 0x5db2d5b5c68e <unknown>\n",
      "#8 0x5db2d5dfda2b <unknown>\n",
      "#9 0x5db2d5e019b1 <unknown>\n",
      "#10 0x5db2d5dea225 <unknown>\n",
      "#11 0x5db2d5e02532 <unknown>\n",
      "#12 0x5db2d5dcf38f <unknown>\n",
      "#13 0x5db2d5e21f28 <unknown>\n",
      "#14 0x5db2d5e220f3 <unknown>\n",
      "#15 0x5db2d5e31e7c <unknown>\n",
      "#16 0x7ea77437bac3 <unknown>\n",
      "\n",
      "Sleeping for 532.68 seconds before retrying... (Attempt 1 of 10)\n",
      "Scraped 497 unique professor IDs so far...\n",
      "Scraped 497 unique professor IDs so far...\n",
      "Scraped 497 unique professor IDs so far...\n",
      "Scraped 498 unique professor IDs so far...\n",
      "Scraped 498 unique professor IDs so far...\n",
      "Scraped 498 unique professor IDs so far...\n",
      "Scraped 498 unique professor IDs so far...\n",
      "Scraped 498 unique professor IDs so far...\n",
      "Scraped 498 unique professor IDs so far...\n",
      "Scraped 498 unique professor IDs so far...\n",
      "Scraped 498 unique professor IDs so far...\n",
      "Scraped 498 unique professor IDs so far...\n",
      "Scraped 498 unique professor IDs so far...\n",
      "Scraped 499 unique professor IDs so far...\n",
      "Scraped 499 unique professor IDs so far...\n",
      "Scraped 500 unique professor IDs so far...\n",
      "Scraped 500 unique professor IDs so far...\n",
      "Scraped 500 unique professor IDs so far...\n",
      "Scraped 500 unique professor IDs so far...\n",
      "Scraped 500 unique professor IDs so far...\n",
      "Scraped 500 unique professor IDs so far...\n",
      "Scraped 500 unique professor IDs so far...\n",
      "Scraped 501 unique professor IDs so far...\n",
      "Scraped 502 unique professor IDs so far...\n",
      "Scraped 502 unique professor IDs so far...\n",
      "Scraped 505 unique professor IDs so far...\n",
      "Scraped 505 unique professor IDs so far...\n",
      "Scraped 506 unique professor IDs so far...\n",
      "Scraped 506 unique professor IDs so far...\n",
      "Scraped 506 unique professor IDs so far...\n",
      "Scraped 507 unique professor IDs so far...\n",
      "Scraped 507 unique professor IDs so far...\n",
      "Scraped 507 unique professor IDs so far...\n",
      "Scraped 507 unique professor IDs so far...\n",
      "Scraped 510 unique professor IDs so far...\n",
      "Scraped 510 unique professor IDs so far...\n",
      "Scraped 510 unique professor IDs so far...\n",
      "Scraped 511 unique professor IDs so far...\n",
      "Scraped 511 unique professor IDs so far...\n",
      "Scraped 516 unique professor IDs so far...\n",
      "Scraped 516 unique professor IDs so far...\n",
      "Scraped 519 unique professor IDs so far...\n",
      "Scraped 519 unique professor IDs so far...\n",
      "Scraped 520 unique professor IDs so far...\n",
      "Scraped 521 unique professor IDs so far...\n",
      "Scraped 521 unique professor IDs so far...\n",
      "Scraped 521 unique professor IDs so far...\n",
      "Scraped 521 unique professor IDs so far...\n",
      "Scraped 521 unique professor IDs so far...\n",
      "Scraped 521 unique professor IDs so far...\n",
      "Scraped 521 unique professor IDs so far...\n",
      "Scraped 521 unique professor IDs so far...\n",
      "Scraped 521 unique professor IDs so far...\n",
      "Scraped 522 unique professor IDs so far...\n",
      "Scraped 525 unique professor IDs so far...\n",
      "Scraped 525 unique professor IDs so far...\n",
      "Scraped 528 unique professor IDs so far...\n",
      "Scraped 528 unique professor IDs so far...\n",
      "Scraped 534 unique professor IDs so far...\n",
      "Scraped 537 unique professor IDs so far...\n",
      "Scraped 537 unique professor IDs so far...\n",
      "Scraped 537 unique professor IDs so far...\n",
      "Scraped 537 unique professor IDs so far...\n",
      "Scraped 540 unique professor IDs so far...\n",
      "Scraped 540 unique professor IDs so far...\n",
      "Scraped 541 unique professor IDs so far...\n",
      "Scraped 545 unique professor IDs so far...\n",
      "Scraped 545 unique professor IDs so far...\n",
      "Scraped 549 unique professor IDs so far...\n",
      "Scraped 557 unique professor IDs so far...\n",
      "Scraped 565 unique professor IDs so far...\n",
      "Scraped 573 unique professor IDs so far...\n",
      "Scraped 575 unique professor IDs so far...\n",
      "Scraped 581 unique professor IDs so far...\n",
      "Scraped 588 unique professor IDs so far...\n",
      "Scraped 596 unique professor IDs so far...\n",
      "Scraped 602 unique professor IDs so far...\n",
      "Scraped 610 unique professor IDs so far...\n",
      "Scraped 618 unique professor IDs so far...\n",
      "Scraped 624 unique professor IDs so far...\n",
      "A timeout occurred: Message: script timeout\n",
      "  (Session info: chrome=129.0.6668.89)\n",
      "Stacktrace:\n",
      "#0 0x5a26daee102a <unknown>\n",
      "#1 0x5a26dabc743d <unknown>\n",
      "#2 0x5a26dac5bd33 <unknown>\n",
      "#3 0x5a26dac3bb22 <unknown>\n",
      "#4 0x5a26dac5ad7d <unknown>\n",
      "#5 0x5a26dac3b8c3 <unknown>\n",
      "#6 0x5a26dac096b3 <unknown>\n",
      "#7 0x5a26dac0a68e <unknown>\n",
      "#8 0x5a26daeaba2b <unknown>\n",
      "#9 0x5a26daeaf9b1 <unknown>\n",
      "#10 0x5a26dae98225 <unknown>\n",
      "#11 0x5a26daeb0532 <unknown>\n",
      "#12 0x5a26dae7d38f <unknown>\n",
      "#13 0x5a26daecff28 <unknown>\n",
      "#14 0x5a26daed00f3 <unknown>\n",
      "#15 0x5a26daedfe7c <unknown>\n",
      "#16 0x79f991aadac3 <unknown>\n",
      "\n",
      "Sleeping for 595.55 seconds before retrying... (Attempt 2 of 10)\n",
      "Scraped 624 unique professor IDs so far...\n",
      "Scraped 624 unique professor IDs so far...\n",
      "Scraped 624 unique professor IDs so far...\n",
      "Scraped 624 unique professor IDs so far...\n",
      "Scraped 624 unique professor IDs so far...\n",
      "Scraped 624 unique professor IDs so far...\n",
      "Scraped 624 unique professor IDs so far...\n",
      "Scraped 624 unique professor IDs so far...\n",
      "Scraped 624 unique professor IDs so far...\n",
      "Scraped 624 unique professor IDs so far...\n",
      "Scraped 624 unique professor IDs so far...\n",
      "Scraped 624 unique professor IDs so far...\n",
      "Scraped 624 unique professor IDs so far...\n",
      "Scraped 624 unique professor IDs so far...\n",
      "Scraped 624 unique professor IDs so far...\n",
      "Scraped 624 unique professor IDs so far...\n",
      "Scraped 624 unique professor IDs so far...\n",
      "Scraped 624 unique professor IDs so far...\n",
      "Scraped 624 unique professor IDs so far...\n",
      "Scraped 624 unique professor IDs so far...\n",
      "Scraped 624 unique professor IDs so far...\n",
      "Scraped 624 unique professor IDs so far...\n",
      "Scraped 624 unique professor IDs so far...\n",
      "Scraped 624 unique professor IDs so far...\n",
      "Scraped 624 unique professor IDs so far...\n",
      "Scraped 624 unique professor IDs so far...\n",
      "Scraped 624 unique professor IDs so far...\n",
      "Scraped 624 unique professor IDs so far...\n",
      "Scraped 624 unique professor IDs so far...\n",
      "Scraped 624 unique professor IDs so far...\n",
      "Scraped 624 unique professor IDs so far...\n",
      "Scraped 624 unique professor IDs so far...\n",
      "Scraped 624 unique professor IDs so far...\n",
      "Scraped 624 unique professor IDs so far...\n",
      "Scraped 624 unique professor IDs so far...\n",
      "Scraped 624 unique professor IDs so far...\n",
      "Scraped 624 unique professor IDs so far...\n",
      "Scraped 624 unique professor IDs so far...\n",
      "Scraped 624 unique professor IDs so far...\n",
      "Scraped 624 unique professor IDs so far...\n",
      "Scraped 624 unique professor IDs so far...\n",
      "Scraped 624 unique professor IDs so far...\n",
      "Scraped 624 unique professor IDs so far...\n",
      "Scraped 624 unique professor IDs so far...\n",
      "Scraped 624 unique professor IDs so far...\n",
      "Scraped 624 unique professor IDs so far...\n",
      "Scraped 624 unique professor IDs so far...\n",
      "Scraped 624 unique professor IDs so far...\n",
      "Scraped 624 unique professor IDs so far...\n",
      "Scraped 624 unique professor IDs so far...\n",
      "Scraped 624 unique professor IDs so far...\n",
      "Scraped 624 unique professor IDs so far...\n",
      "Scraped 624 unique professor IDs so far...\n",
      "Scraped 624 unique professor IDs so far...\n",
      "Scraped 624 unique professor IDs so far...\n",
      "Scraped 624 unique professor IDs so far...\n",
      "Scraped 624 unique professor IDs so far...\n",
      "Scraped 624 unique professor IDs so far...\n",
      "Scraped 624 unique professor IDs so far...\n",
      "Scraped 624 unique professor IDs so far...\n",
      "Scraped 624 unique professor IDs so far...\n",
      "Scraped 624 unique professor IDs so far...\n",
      "Scraped 625 unique professor IDs so far...\n",
      "Scraped 625 unique professor IDs so far...\n",
      "Scraped 625 unique professor IDs so far...\n",
      "Scraped 625 unique professor IDs so far...\n",
      "Scraped 625 unique professor IDs so far...\n",
      "Scraped 627 unique professor IDs so far...\n",
      "Scraped 627 unique professor IDs so far...\n",
      "Scraped 627 unique professor IDs so far...\n",
      "Scraped 629 unique professor IDs so far...\n",
      "Scraped 631 unique professor IDs so far...\n",
      "Scraped 635 unique professor IDs so far...\n",
      "Scraped 636 unique professor IDs so far...\n",
      "Scraped 636 unique professor IDs so far...\n",
      "Scraped 636 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "A timeout occurred: Message: script timeout\n",
      "  (Session info: chrome=129.0.6668.89)\n",
      "Stacktrace:\n",
      "#0 0x58b53f5e202a <unknown>\n",
      "#1 0x58b53f2c843d <unknown>\n",
      "#2 0x58b53f35cd33 <unknown>\n",
      "#3 0x58b53f33cb22 <unknown>\n",
      "#4 0x58b53f35bd7d <unknown>\n",
      "#5 0x58b53f33c8c3 <unknown>\n",
      "#6 0x58b53f30a6b3 <unknown>\n",
      "#7 0x58b53f30b68e <unknown>\n",
      "#8 0x58b53f5aca2b <unknown>\n",
      "#9 0x58b53f5b09b1 <unknown>\n",
      "#10 0x58b53f599225 <unknown>\n",
      "#11 0x58b53f5b1532 <unknown>\n",
      "#12 0x58b53f57e38f <unknown>\n",
      "#13 0x58b53f5d0f28 <unknown>\n",
      "#14 0x58b53f5d10f3 <unknown>\n",
      "#15 0x58b53f5e0e7c <unknown>\n",
      "#16 0x7678c7006ac3 <unknown>\n",
      "\n",
      "Sleeping for 501.92 seconds before retrying... (Attempt 3 of 10)\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "A timeout occurred: Message: script timeout\n",
      "  (Session info: chrome=129.0.6668.89)\n",
      "Stacktrace:\n",
      "#0 0x62f3247d102a <unknown>\n",
      "#1 0x62f3244b743d <unknown>\n",
      "#2 0x62f32454bd33 <unknown>\n",
      "#3 0x62f32452bb22 <unknown>\n",
      "#4 0x62f32454ad7d <unknown>\n",
      "#5 0x62f32452b8c3 <unknown>\n",
      "#6 0x62f3244f96b3 <unknown>\n",
      "#7 0x62f3244fa68e <unknown>\n",
      "#8 0x62f32479ba2b <unknown>\n",
      "#9 0x62f32479f9b1 <unknown>\n",
      "#10 0x62f324788225 <unknown>\n",
      "#11 0x62f3247a0532 <unknown>\n",
      "#12 0x62f32476d38f <unknown>\n",
      "#13 0x62f3247bff28 <unknown>\n",
      "#14 0x62f3247c00f3 <unknown>\n",
      "#15 0x62f3247cfe7c <unknown>\n",
      "#16 0x760999eb4ac3 <unknown>\n",
      "\n",
      "Sleeping for 405.43 seconds before retrying... (Attempt 4 of 10)\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "A timeout occurred: Message: script timeout\n",
      "  (Session info: chrome=129.0.6668.89)\n",
      "Stacktrace:\n",
      "#0 0x5ef5cb75702a <unknown>\n",
      "#1 0x5ef5cb43d43d <unknown>\n",
      "#2 0x5ef5cb4d1d33 <unknown>\n",
      "#3 0x5ef5cb4b1b22 <unknown>\n",
      "#4 0x5ef5cb4d0d7d <unknown>\n",
      "#5 0x5ef5cb4b18c3 <unknown>\n",
      "#6 0x5ef5cb47f6b3 <unknown>\n",
      "#7 0x5ef5cb48068e <unknown>\n",
      "#8 0x5ef5cb721a2b <unknown>\n",
      "#9 0x5ef5cb7259b1 <unknown>\n",
      "#10 0x5ef5cb70e225 <unknown>\n",
      "#11 0x5ef5cb726532 <unknown>\n",
      "#12 0x5ef5cb6f338f <unknown>\n",
      "#13 0x5ef5cb745f28 <unknown>\n",
      "#14 0x5ef5cb7460f3 <unknown>\n",
      "#15 0x5ef5cb755e7c <unknown>\n",
      "#16 0x7730aaafbac3 <unknown>\n",
      "\n",
      "Sleeping for 557.22 seconds before retrying... (Attempt 5 of 10)\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "A timeout occurred: Message: script timeout\n",
      "  (Session info: chrome=129.0.6668.89)\n",
      "Stacktrace:\n",
      "#0 0x6293717fe02a <unknown>\n",
      "#1 0x6293714e443d <unknown>\n",
      "#2 0x629371578d33 <unknown>\n",
      "#3 0x629371558b22 <unknown>\n",
      "#4 0x629371577d7d <unknown>\n",
      "#5 0x6293715588c3 <unknown>\n",
      "#6 0x6293715266b3 <unknown>\n",
      "#7 0x62937152768e <unknown>\n",
      "#8 0x6293717c8a2b <unknown>\n",
      "#9 0x6293717cc9b1 <unknown>\n",
      "#10 0x6293717b5225 <unknown>\n",
      "#11 0x6293717cd532 <unknown>\n",
      "#12 0x62937179a38f <unknown>\n",
      "#13 0x6293717ecf28 <unknown>\n",
      "#14 0x6293717ed0f3 <unknown>\n",
      "#15 0x6293717fce7c <unknown>\n",
      "#16 0x7f1c781faac3 <unknown>\n",
      "\n",
      "Sleeping for 453.95 seconds before retrying... (Attempt 6 of 10)\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "A timeout occurred: Message: script timeout\n",
      "  (Session info: chrome=129.0.6668.89)\n",
      "Stacktrace:\n",
      "#0 0x64bb15c8302a <unknown>\n",
      "#1 0x64bb1596943d <unknown>\n",
      "#2 0x64bb159fdd33 <unknown>\n",
      "#3 0x64bb159ddb22 <unknown>\n",
      "#4 0x64bb159fcd7d <unknown>\n",
      "#5 0x64bb159dd8c3 <unknown>\n",
      "#6 0x64bb159ab6b3 <unknown>\n",
      "#7 0x64bb159ac68e <unknown>\n",
      "#8 0x64bb15c4da2b <unknown>\n",
      "#9 0x64bb15c519b1 <unknown>\n",
      "#10 0x64bb15c3a225 <unknown>\n",
      "#11 0x64bb15c52532 <unknown>\n",
      "#12 0x64bb15c1f38f <unknown>\n",
      "#13 0x64bb15c71f28 <unknown>\n",
      "#14 0x64bb15c720f3 <unknown>\n",
      "#15 0x64bb15c81e7c <unknown>\n",
      "#16 0x761ca30d2ac3 <unknown>\n",
      "\n",
      "Sleeping for 358.84 seconds before retrying... (Attempt 7 of 10)\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "A timeout occurred: Message: script timeout\n",
      "  (Session info: chrome=129.0.6668.89)\n",
      "Stacktrace:\n",
      "#0 0x605653efb02a <unknown>\n",
      "#1 0x605653be143d <unknown>\n",
      "#2 0x605653c75d33 <unknown>\n",
      "#3 0x605653c55b22 <unknown>\n",
      "#4 0x605653c74d7d <unknown>\n",
      "#5 0x605653c558c3 <unknown>\n",
      "#6 0x605653c236b3 <unknown>\n",
      "#7 0x605653c2468e <unknown>\n",
      "#8 0x605653ec5a2b <unknown>\n",
      "#9 0x605653ec99b1 <unknown>\n",
      "#10 0x605653eb2225 <unknown>\n",
      "#11 0x605653eca532 <unknown>\n",
      "#12 0x605653e9738f <unknown>\n",
      "#13 0x605653ee9f28 <unknown>\n",
      "#14 0x605653eea0f3 <unknown>\n",
      "#15 0x605653ef9e7c <unknown>\n",
      "#16 0x7d5671225ac3 <unknown>\n",
      "\n",
      "Sleeping for 574.66 seconds before retrying... (Attempt 8 of 10)\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "A timeout occurred: Message: script timeout\n",
      "  (Session info: chrome=129.0.6668.89)\n",
      "Stacktrace:\n",
      "#0 0x631b9947b02a <unknown>\n",
      "#1 0x631b9916143d <unknown>\n",
      "#2 0x631b991f5d33 <unknown>\n",
      "#3 0x631b991d5b22 <unknown>\n",
      "#4 0x631b991f4d7d <unknown>\n",
      "#5 0x631b991d58c3 <unknown>\n",
      "#6 0x631b991a36b3 <unknown>\n",
      "#7 0x631b991a468e <unknown>\n",
      "#8 0x631b99445a2b <unknown>\n",
      "#9 0x631b994499b1 <unknown>\n",
      "#10 0x631b99432225 <unknown>\n",
      "#11 0x631b9944a532 <unknown>\n",
      "#12 0x631b9941738f <unknown>\n",
      "#13 0x631b99469f28 <unknown>\n",
      "#14 0x631b9946a0f3 <unknown>\n",
      "#15 0x631b99479e7c <unknown>\n",
      "#16 0x703de57b8ac3 <unknown>\n",
      "\n",
      "Sleeping for 575.89 seconds before retrying... (Attempt 9 of 10)\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "Scraped 638 unique professor IDs so far...\n",
      "A timeout occurred: Message: script timeout\n",
      "  (Session info: chrome=129.0.6668.89)\n",
      "Stacktrace:\n",
      "#0 0x5dce5d09e02a <unknown>\n",
      "#1 0x5dce5cd8443d <unknown>\n",
      "#2 0x5dce5ce18d33 <unknown>\n",
      "#3 0x5dce5cdf8b22 <unknown>\n",
      "#4 0x5dce5ce17d7d <unknown>\n",
      "#5 0x5dce5cdf88c3 <unknown>\n",
      "#6 0x5dce5cdc66b3 <unknown>\n",
      "#7 0x5dce5cdc768e <unknown>\n",
      "#8 0x5dce5d068a2b <unknown>\n",
      "#9 0x5dce5d06c9b1 <unknown>\n",
      "#10 0x5dce5d055225 <unknown>\n",
      "#11 0x5dce5d06d532 <unknown>\n",
      "#12 0x5dce5d03a38f <unknown>\n",
      "#13 0x5dce5d08cf28 <unknown>\n",
      "#14 0x5dce5d08d0f3 <unknown>\n",
      "#15 0x5dce5d09ce7c <unknown>\n",
      "#16 0x793feca1bac3 <unknown>\n",
      "\n",
      "Sleeping for 541.03 seconds before retrying... (Attempt 10 of 10)\n",
      "Max retries reached. Some data may be missing.\n",
      "\n",
      "Total unique professor IDs scraped: 638\n",
      "Sample of professor IDs:\n",
      "['2317131', '2368815', '746627', '2301109', '1110032', '1163980', '1185695', '612323', '1638295', '2294669']\n",
      "\n",
      "Sample of professor URLs:\n",
      "['https://www.ratemyprofessors.com/professor/2317131', 'https://www.ratemyprofessors.com/professor/2368815', 'https://www.ratemyprofessors.com/professor/746627', 'https://www.ratemyprofessors.com/professor/2301109', 'https://www.ratemyprofessors.com/professor/1110032']\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException, WebDriverException\n",
    "import time\n",
    "import re\n",
    "import random\n",
    "\n",
    "def get_professor_ids(url, max_retries=10):\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")  # Run in headless mode\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "    professor_ids = set()\n",
    "    retry_count = 0\n",
    "\n",
    "    while retry_count < max_retries:\n",
    "        driver = webdriver.Chrome(options=chrome_options)\n",
    "        driver.set_page_load_timeout(30)  # Set page load timeout\n",
    "\n",
    "        try:\n",
    "            driver.get(url)\n",
    "            while True:\n",
    "                # Wait for the professor cards to load\n",
    "                WebDriverWait(driver, 20).until(\n",
    "                    EC.presence_of_element_located((By.CLASS_NAME, \"TeacherCard__StyledTeacherCard-syjs0d-0\"))\n",
    "                )\n",
    "                \n",
    "                # Extract professor IDs from the current page\n",
    "                cards = driver.find_elements(By.CLASS_NAME, \"TeacherCard__StyledTeacherCard-syjs0d-0\")\n",
    "                for card in cards:\n",
    "                    href = card.get_attribute('href')\n",
    "                    if href:\n",
    "                        match = re.search(r'/professor/(\\d+)', href)\n",
    "                        if match:\n",
    "                            professor_ids.add(match.group(1))\n",
    "                \n",
    "                print(f\"Scraped {len(professor_ids)} unique professor IDs so far...\")\n",
    "                \n",
    "                try:\n",
    "                    # Try to click the \"Show More\" button\n",
    "                    show_more_button = WebDriverWait(driver, 20).until(\n",
    "                        EC.element_to_be_clickable((By.CLASS_NAME, \"Buttons__Button-sc-19xdot-1\"))\n",
    "                    )\n",
    "                    driver.execute_script(\"arguments[0].click();\", show_more_button)\n",
    "                    time.sleep(random.uniform(2, 5))  # Random delay between 2 and 5 seconds\n",
    "                except TimeoutException:\n",
    "                    print(\"No more 'Show More' button found. Finished scraping.\")\n",
    "                    break\n",
    "\n",
    "            # If we've made it here without exceptions, we're done\n",
    "            break\n",
    "\n",
    "        except TimeoutException as e:\n",
    "            print(f\"A timeout occurred: {e}\")\n",
    "            retry_count += 1\n",
    "            sleep_time = random.uniform(300, 600)  # Sleep for 5-10 minutes (300-600 seconds)\n",
    "            print(f\"Sleeping for {sleep_time:.2f} seconds before retrying... (Attempt {retry_count} of {max_retries})\")\n",
    "            time.sleep(sleep_time)\n",
    "\n",
    "        except WebDriverException as e:\n",
    "            print(f\"A WebDriver error occurred: {e}\")\n",
    "            retry_count += 1\n",
    "            print(f\"Retrying... (Attempt {retry_count} of {max_retries})\")\n",
    "            time.sleep(random.uniform(10, 20))  # Random delay between retries\n",
    "\n",
    "        finally:\n",
    "            driver.quit()\n",
    "\n",
    "    if retry_count == max_retries:\n",
    "        print(\"Max retries reached. Some data may be missing.\")\n",
    "\n",
    "    return list(professor_ids)\n",
    "\n",
    "# Usage\n",
    "url = \"https://www.ratemyprofessors.com/search/professors/481?q=*\"\n",
    "professor_ids = get_professor_ids(url)\n",
    "\n",
    "print(f\"\\nTotal unique professor IDs scraped: {len(professor_ids)}\")\n",
    "print(\"Sample of professor IDs:\")\n",
    "print(professor_ids[:10])\n",
    "\n",
    "# Generate full URLs\n",
    "base_url = \"https://www.ratemyprofessors.com/professor/\"\n",
    "professor_urls = [f\"{base_url}{id}\" for id in professor_ids]\n",
    "\n",
    "print(\"\\nSample of professor URLs:\")\n",
    "print(professor_urls[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fb6d0385-93e3-48e1-b92b-5422013f9bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 639 URLs to professor_urls.txt\n"
     ]
    }
   ],
   "source": [
    "def save_urls_to_file(urls, filename):\n",
    "    \"\"\"\n",
    "    Save a list of URLs to a text file, with each URL on a new line.\n",
    "    \n",
    "    Args:\n",
    "    urls (list): List of URLs to save\n",
    "    filename (str): Name of the file to save the URLs to\n",
    "    \"\"\"\n",
    "    with open(filename, 'w') as file:\n",
    "        for url in urls:\n",
    "            file.write(f\"{url}\\n\")\n",
    "    print(f\"Saved {len(urls)} URLs to {filename}\")\n",
    "\n",
    "# Usage\n",
    "filename = \"professor_urls.txt\"\n",
    "save_urls_to_file(professor_urls, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f7295739-1b84-4938-af36-b20e2c5ceeff",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'string'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 51\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m     50\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.ratemyprofessors.com/professor/1512959\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 51\u001b[0m professor_document \u001b[38;5;241m=\u001b[39m \u001b[43mextract_professor_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(json\u001b[38;5;241m.\u001b[39mdumps(professor_document, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m))\n",
      "Cell \u001b[0;32mIn[26], line 7\u001b[0m, in \u001b[0;36mextract_professor_info\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Extract data from the JSON-LD script tag\u001b[39;00m\n\u001b[1;32m      6\u001b[0m script_tag \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscript\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__NEXT_DATA__\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[0;32m----> 7\u001b[0m data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(\u001b[43mscript_tag\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstring\u001b[49m)\n\u001b[1;32m      8\u001b[0m teacher_data \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprops\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpageProps\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mteacherRatings\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnode\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Extract relevant information\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'string'"
     ]
    }
   ],
   "source": [
    "def extract_professor_info(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Extract data from the JSON-LD script tag\n",
    "    script_tag = soup.find('script', {'id': '__NEXT_DATA__'})\n",
    "    data = json.loads(script_tag.string)\n",
    "    teacher_data = data['props']['pageProps']['teacherRatings']['node']\n",
    "    \n",
    "    # Extract relevant information\n",
    "    name = f\"{teacher_data['firstName']} {teacher_data['lastName']}\"\n",
    "    department = teacher_data['department']\n",
    "    school = teacher_data['school']['name']\n",
    "    overall_quality = teacher_data['avgRating']\n",
    "    num_ratings = teacher_data['numRatings']\n",
    "    would_take_again = teacher_data['wouldTakeAgainPercent']\n",
    "    level_of_difficulty = teacher_data['avgDifficulty']\n",
    "    \n",
    "    # Extract top tags\n",
    "    top_tags = [tag['tagName'] for tag in teacher_data['teacherRatingTags']]\n",
    "    \n",
    "    # Extract individual ratings (limited to 5 for brevity)\n",
    "    ratings = []\n",
    "    for rating in teacher_data['ratings']['edges'][:5]:\n",
    "        rating_data = rating['node']\n",
    "        ratings.append({\n",
    "            'class': rating_data['class'],\n",
    "            'date': rating_data['date'],\n",
    "            'quality': rating_data['helpfulRating'],\n",
    "            'difficulty': rating_data['difficultyRating'],\n",
    "            'comment': rating_data['comment']\n",
    "        })\n",
    "    \n",
    "    # Create structured document\n",
    "    document = {\n",
    "        'name': name,\n",
    "        'department': department,\n",
    "        'school': school,\n",
    "        'overall_quality': overall_quality,\n",
    "        'num_ratings': num_ratings,\n",
    "        'would_take_again': would_take_again,\n",
    "        'level_of_difficulty': level_of_difficulty,\n",
    "        'top_tags': top_tags,\n",
    "        'sample_ratings': ratings\n",
    "    }\n",
    "    \n",
    "    return document\n",
    "\n",
    "# Example usage\n",
    "url = \"https://www.ratemyprofessors.com/professor/1512959\"\n",
    "professor_document = extract_professor_info(url)\n",
    "print(json.dumps(professor_document, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fe5bb68f-65ea-4f57-beeb-344e3ed4c8b4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GPT4AllEmbeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 103\u001b[0m\n\u001b[1;32m    100\u001b[0m professor_urls \u001b[38;5;241m=\u001b[39m load_professor_urls(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprofessor_urls.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# Initialize the embedding model\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mGPT4AllEmbeddings\u001b[49m()\n\u001b[1;32m    105\u001b[0m \u001b[38;5;66;03m# Initialize Chroma vectorstore\u001b[39;00m\n\u001b[1;32m    106\u001b[0m vectorstore \u001b[38;5;241m=\u001b[39m Chroma(embedding_function\u001b[38;5;241m=\u001b[39membeddings, persist_directory\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./chroma_db\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'GPT4AllEmbeddings' is not defined"
     ]
    }
   ],
   "source": [
    "def extract_professor_info(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    script_tag = soup.find('script', {'id': '__NEXT_DATA__'})\n",
    "    data = json.loads(script_tag.string)\n",
    "    teacher_data = data['props']['pageProps']['teacherRatings']['node']\n",
    "    \n",
    "    name = f\"{teacher_data['firstName']} {teacher_data['lastName']}\"\n",
    "    department = teacher_data['department']\n",
    "    school = teacher_data['school']['name']\n",
    "    overall_quality = teacher_data['avgRating']\n",
    "    num_ratings = teacher_data['numRatings']\n",
    "    would_take_again = teacher_data['wouldTakeAgainPercent']\n",
    "    level_of_difficulty = teacher_data['avgDifficulty']\n",
    "    \n",
    "    top_tags = [tag['tagName'] for tag in teacher_data['teacherRatingTags']]\n",
    "    \n",
    "    ratings = []\n",
    "    for rating in teacher_data['ratings']['edges'][:10]:\n",
    "        rating_data = rating['node']\n",
    "        ratings.append({\n",
    "            'class': rating_data['class'],\n",
    "            'date': rating_data['date'],\n",
    "            'quality': rating_data['helpfulRating'],\n",
    "            'difficulty': rating_data['difficultyRating'],\n",
    "            'comment': rating_data['comment']\n",
    "        })\n",
    "    \n",
    "    document = {\n",
    "        'name': name,\n",
    "        'department': department,\n",
    "        'school': school,\n",
    "        'overall_quality': overall_quality,\n",
    "        'num_ratings': num_ratings,\n",
    "        'would_take_again': would_take_again,\n",
    "        'level_of_difficulty': level_of_difficulty,\n",
    "        'top_tags': top_tags,\n",
    "        'sample_ratings': ratings,\n",
    "        'url': url\n",
    "    }\n",
    "    \n",
    "    return document\n",
    "\n",
    "def document_to_text(doc):\n",
    "    text = f\"{doc['name']} is a professor in the {doc['department']} department at {doc['school']}. \"\n",
    "    text += f\"Overall quality: {doc['overall_quality']}/5 based on {doc['num_ratings']} ratings. \"\n",
    "    text += f\"{doc['would_take_again']}% would take again. Level of difficulty: {doc['level_of_difficulty']}/5. \"\n",
    "    text += f\"Top tags: {', '.join(doc['top_tags'])}. \"\n",
    "    for rating in doc['sample_ratings']:\n",
    "        text += f\"Sample rating: {rating['quality']}/5, {rating['difficulty']}/5 difficulty, {rating['comment']} \"\n",
    "    return text\n",
    "\n",
    "def load_professor_urls(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        return [line.strip() for line in file]\n",
    "\n",
    "def index_professors(urls, embeddings, vectorstore):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    \n",
    "    for url in tqdm(urls, desc=\"Indexing professors\"):\n",
    "        try:\n",
    "            # Extract professor info\n",
    "            prof_doc = extract_professor_info(url)\n",
    "            \n",
    "            # Convert to text\n",
    "            prof_text = document_to_text(prof_doc)\n",
    "            \n",
    "            # Split text into chunks\n",
    "            chunks = text_splitter.split_text(prof_text)\n",
    "            \n",
    "            # Create Langchain documents\n",
    "            documents = [\n",
    "                Document(\n",
    "                    page_content=chunk,\n",
    "                    metadata={\n",
    "                        \"name\": prof_doc['name'],\n",
    "                        \"department\": prof_doc['department'],\n",
    "                        \"school\": prof_doc['school'],\n",
    "                        \"overall_quality\": str(prof_doc['overall_quality']),\n",
    "                        \"num_ratings\": str(prof_doc['num_ratings']),\n",
    "                        \"would_take_again\": str(prof_doc['would_take_again']),\n",
    "                        \"level_of_difficulty\": str(prof_doc['level_of_difficulty']),\n",
    "                        \"url\": url\n",
    "                    }\n",
    "                ) for chunk in chunks\n",
    "            ]\n",
    "            \n",
    "            # Add to vectorstore\n",
    "            vectorstore.add_documents(documents)\n",
    "            \n",
    "            # Sleep to avoid rate limiting\n",
    "            time.sleep(1)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {url}: {str(e)}\")\n",
    "\n",
    "# Usage\n",
    "# Load professor URLs\n",
    "professor_urls = load_professor_urls(\"professor_urls.txt\")\n",
    "    \n",
    "# Initialize the embedding model\n",
    "embeddings = GPT4AllEmbeddings()\n",
    "    \n",
    "# Initialize Chroma vectorstore\n",
    "vectorstore = Chroma(embedding_function=embeddings, persist_directory=\"./chroma_db\")\n",
    "    \n",
    "# Run indexing process\n",
    "index_professors(professor_urls, embeddings, vectorstore)\n",
    "    \n",
    "# Persist the vectorstore\n",
    "vectorstore.persist()\n",
    "    \n",
    "# Verify indexing\n",
    "print(f\"Total documents indexed: {vectorstore._collection.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccef55c-a08e-4407-8503-487302c92df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Dict\n",
    "from langchain_community.document_loaders import UnstructuredLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_pdf(file_path: str) -> List[Document]:\n",
    "    loader = UnstructuredLoader(\n",
    "        file_path=file_path,\n",
    "        strategy=\"hi_res\",\n",
    "        coordinates=True,\n",
    "    )\n",
    "    return list(loader.lazy_load())\n",
    "\n",
    "def process_pdfs(directory: str) -> List[Document]:\n",
    "    documents = []\n",
    "    for filename in tqdm(os.listdir(directory), desc=\"Processing PDFs\"):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            docs = load_pdf(file_path)\n",
    "            # Add filename to metadata for better structure\n",
    "            for doc in docs:\n",
    "                doc.metadata[\"source_file\"] = filename\n",
    "            documents.extend(docs)\n",
    "    return documents\n",
    "\n",
    "def split_documents(documents: List[Document]) -> List[Document]:\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "    )\n",
    "    return text_splitter.split_documents(documents)\n",
    "\n",
    "def load_course_info(directory: str) -> Dict[str, Dict[str, str]]:\n",
    "    course_info = {}\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            with open(os.path.join(directory, filename), 'r') as f:\n",
    "                lines = f.readlines()\n",
    "                for line in lines:\n",
    "                    parts = line.strip().split(',')\n",
    "                    if len(parts) == 3:\n",
    "                        course_number, course_name, description = parts\n",
    "                        course_info[course_number] = {\n",
    "                            \"name\": course_name,\n",
    "                            \"description\": description\n",
    "                        }\n",
    "    return course_info\n",
    "\n",
    "def enrich_documents(documents: List[Document], course_info: Dict[str, Dict[str, str]]) -> List[Document]:\n",
    "    enriched_docs = []\n",
    "    for doc in documents:\n",
    "        # Check if the document contains a course number\n",
    "        for course_number in course_info.keys():\n",
    "            if course_number in doc.page_content:\n",
    "                doc.metadata[\"course_number\"] = course_number\n",
    "                doc.metadata[\"course_name\"] = course_info[course_number][\"name\"]\n",
    "                doc.metadata[\"course_description\"] = course_info[course_number][\"description\"]\n",
    "                break\n",
    "        enriched_docs.append(doc)\n",
    "    return enriched_docs\n",
    "\n",
    "def index_documents(documents: List[Document]):\n",
    "    embeddings = GPT4AllEmbeddings()\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=documents,\n",
    "        embedding=embeddings,\n",
    "        persist_directory=\"./ksu_catalog_db\"\n",
    "    )\n",
    "    vectorstore.persist()\n",
    "    return vectorstore\n",
    "\n",
    "def main():\n",
    "ksu_data_dir = \"./ksu_data\"\n",
    "pdf_directory = os.path.join(ksu_data_dir, \"undergraduate_catalogs\")\n",
    "course_info_directory = os.path.join(ksu_data_dir, \"undergraduate_categories\")\n",
    "    \n",
    "print(\"Loading course information...\")\n",
    "course_info = load_course_info(course_info_directory)\n",
    "    \n",
    "print(\"Loading and processing PDFs...\")\n",
    "documents = process_pdfs(pdf_directory)\n",
    "    \n",
    "print(\"Splitting documents...\")\n",
    "split_docs = split_documents(documents)\n",
    "    \n",
    "print(\"Enriching documents with course information...\")\n",
    "enriched_docs = enrich_documents(split_docs, course_info)\n",
    "    \n",
    "print(\"Indexing documents...\")\n",
    "vectorstore = index_documents(enriched_docs)\n",
    "    \n",
    "print(f\"Indexed {len(enriched_docs)} document chunks.\")\n",
    "print(f\"Vector store persisted at ./ksu_catalog_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05ef44c-7eba-4a94-a33c-7c4d1639fa43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1af1d3-ba9e-415e-ade2-fbcd1f7f8388",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311fbacd-1846-42c2-be80-8e169e7c6ce5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (scrappy_venv)",
   "language": "python",
   "name": ".scrappy_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
